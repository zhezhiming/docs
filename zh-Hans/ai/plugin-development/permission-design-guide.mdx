---
sidebar_position: 10
title: æ’ä»¶æƒé™è®¾è®¡æŒ‡å—
---

:::tip
åœ¨ meta ä¸­è®¾è®¡ä¸€å¥—å¯ä»¥å‘æ ¸å¿ƒç³»ç»Ÿåå‘è¯·æ±‚ å¤§æ¨¡å‹ è°ƒç”¨æƒé™çš„ï¼ˆå¦‚è¯·æ±‚ä¸€ä¸ª OpenAI Clientå®ä¾‹ï¼‰çš„é…ç½®ï¼Œä¾‹å¦‚åœ¨ metaä¸­å£°æ˜å¤§æ¨¡å‹çš„æƒé™è¯·æ±‚ï¼Œåœ¨ configSchema ä¸­è®¾è®¡å±æ€§è®©ç”¨æˆ·é…ç½®å…·ä½“çš„æ¨¡å‹æä¾›å•†å’Œæ¨¡å‹åç§°ï¼Œæ ¸å¿ƒç³»ç»Ÿåœ¨è¯†åˆ«åˆ°æƒé™è¯·æ±‚æ—¶ä¼šæ ¹æ® config ä¸­ç”¨æˆ·é…ç½®çš„å…·ä½“å‚æ•°å®ä¾‹åŒ–å¤§æ¨¡å‹ Client é€šè¿‡ config ä¼ å…¥åˆ° understandImages è¿™ä¸ªæ–¹æ³•ï¼š
:::

ğŸ‘Œæ˜ç™½äº†ï¼Œä½ å¸Œæœ› **åœ¨æ’ä»¶ Strategy çš„ meta ä¸­å£°æ˜ä¾èµ–çš„å¤§æ¨¡å‹æƒé™è¯·æ±‚**ï¼Œæ ¸å¿ƒç³»ç»Ÿè§£æåæ ¹æ®ç”¨æˆ·é…ç½®ï¼ˆæ¨¡å‹æä¾›å•† + æ¨¡å‹åç§° + å…¶ä»–å‚æ•°ï¼‰è‡ªåŠ¨æ³¨å…¥ä¸€ä¸ª `client` åˆ° `config`ï¼Œè¿™æ ·æ’ä»¶ä¸ç”¨è‡ªå·±åˆ›å»º OpenAI / Anthropic / å…¶ä»– VLM å®¢æˆ·ç«¯ã€‚

æˆ‘å¸®ä½ é‡æ–°è®¾è®¡ `OpenAIGPT4VStrategy` çš„ meta å’Œ configSchemaï¼Œä½¿å®ƒèƒ½ **åå‘è¯·æ±‚æ ¸å¿ƒç³»ç»Ÿæä¾›çš„ Client**ã€‚

---

## æ–°ç‰ˆ Strategy

```ts
import { Injectable } from '@nestjs/common';
import { Document } from 'langchain/document';
import {
  ImageUnderstandingStrategy,
  IImageUnderstandingStrategy,
  TImageUnderstandingFile,
  TImageUnderstandingResult
} from './strategy.interface';

@Injectable()
@ImageUnderstandingStrategy('openai-gpt4v')
export class OpenAIGPT4VStrategy implements IImageUnderstandingStrategy<any> {
  readonly meta = {
    name: 'openai-gpt4v',
    label: { en: 'OpenAI GPT-4 Vision', zh: 'OpenAI GPT-4 å›¾åƒç†è§£' },

    /**
     * æƒé™å£°æ˜ï¼šå‘Šè¯‰æ ¸å¿ƒç³»ç»Ÿï¼Œè¿™ä¸ªæ’ä»¶éœ€è¦ä¸€ä¸ª "llm" æƒé™ï¼Œ
     * æä¾›æ–¹æ˜¯ openaiï¼Œèƒ½åŠ›æ˜¯ visionã€‚
     * æ ¸å¿ƒç³»ç»Ÿè¯†åˆ«åä¼šæ ¹æ® config.provider + config.model å®ä¾‹åŒ–å¯¹åº”çš„ Clientï¼Œ
     * å¹¶æ³¨å…¥åˆ° config.client
     */
    permissions: [
      {
        type: 'llm',
        provider: 'openai',
        capability: 'vision',
      },
    ],

    /**
     * é…ç½® schemaï¼Œç”¨æˆ·éœ€è¦å¡«å†™ provider å’Œ modelï¼Œ
     * æ ¸å¿ƒç³»ç»Ÿä¼šæ ¹æ® provider + model + ç”¨æˆ·è´¦å· API Keyï¼Œ
     * å®ä¾‹åŒ–å¥½ Client æ³¨å…¥ã€‚
     */
    configSchema: {
      type: 'object',
      properties: {
        provider: { 
          type: 'string', 
          enum: ['openai'], 
          description: 'æ¨¡å‹æä¾›å•†' 
        },
        model: { 
          type: 'string', 
          default: 'gpt-4-vision-preview', 
          description: 'å¤§æ¨¡å‹åç§°' 
        },
        prompt: { 
          type: 'string', 
          default: 'Describe this image in detail.', 
          description: 'æç¤ºè¯ï¼ˆå¯ä»¥è‡ªå®šä¹‰ç†è§£ä»»åŠ¡ï¼‰' 
        }
      },
      required: ['provider', 'model']
    },

    icon: {
      svg: '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20"><circle cx="10" cy="10" r="8" stroke="black" fill="none"/><text x="6" y="14">VLM</text></svg>',
      color: '#10a37f'
    }
  };

  async validateConfig(config: any): Promise<void> {
    if (!config.client) {
      throw new Error('Missing injected LLM client in config. Core system must inject client.');
    }
  }

  async understandImages(
    files: TImageUnderstandingFile[],
    config: any
  ): Promise<TImageUnderstandingResult[]> {
    const client = config.client; // âœ… å·²ç”±æ ¸å¿ƒç³»ç»Ÿæ³¨å…¥
    const results: TImageUnderstandingResult[] = [];

    for (const file of files) {
      const description = await this.runGPT4V(client, file.path, config);

      const doc = new Document({
        pageContent: description,
        metadata: {
          chunkId: `img-${Date.now()}-${Math.random().toString(36).slice(2, 8)}`,
          parentChunkId: file.parentChunkId,
          imagePath: file.path,
          source: file.filename,
          type: 'vlm',
          engine: config.model
        }
      });

      results.push({
        docs: [doc],
        metadata: { engine: config.model, file: file.filename }
      });
    }

    return results;
  }

  private async runGPT4V(client: any, imagePath: string, config: any): Promise<string> {
    // client å·²ç»æ˜¯ openai clientï¼Œç”±æ ¸å¿ƒç³»ç»Ÿå®ä¾‹åŒ–
    const response = await client.chat.completions.create({
      model: config.model,
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: config.prompt || 'Describe this image in detail.' },
            { type: 'image_url', image_url: { url: imagePath } }
          ]
        }
      ],
      max_tokens: 500
    });

    return response.choices[0]?.message?.content || '';
  }
}
```

---

## ğŸ”‘ æ ¸å¿ƒç³»ç»Ÿå¦‚ä½•å¤„ç†

* è§£ææ’ä»¶çš„ `meta.permissions`ï¼Œå‘ç°éœ€è¦ï¼š

  ```json
  { "type": "llm", "provider": "openai", "capability": "vision" }
  ```
* æ ¹æ®ç”¨æˆ· `config.provider = "openai"`, `config.model = "gpt-4-vision-preview"`
  å®ä¾‹åŒ– `OpenAI({ apiKey: userApiKey })`
* å°† client æ³¨å…¥åˆ° `config.client`
* è°ƒç”¨ `strategy.understandImages(files, config)`

---

## ğŸ“Œ ä½¿ç”¨ç¤ºä¾‹

```ts
const strategy = this.imageUnderstandingRegistry.get('openai-gpt4v');
const result = await strategy.understandImages(
  [
    { 
      path: 'https://example.com/chart.png', 
      filename: 'chart.png', 
      extname: 'png', 
      parentChunkId: 'uuid-123' 
    }
  ],
  { 
    provider: 'openai',
    model: 'gpt-4-vision-preview',
    prompt: 'è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€‚',
    client: openaiClient // âœ… æ ¸å¿ƒç³»ç»Ÿæ³¨å…¥
  }
);

console.log(result[0].docs[0].pageContent);
```

---

è¿™æ ·æ’ä»¶æœ¬èº«ä¸ä¾èµ–ä»»ä½• `openai` åŒ…è£…é€»è¾‘ï¼Œ**åªå£°æ˜æƒé™ + è¦æ±‚ provider/model**ï¼Œæ ¸å¿ƒç³»ç»Ÿå°±èƒ½æ³¨å…¥åˆé€‚çš„ clientã€‚

è¦ä¸è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ª **æŠ½è±¡çš„æƒé™å£°æ˜æ¥å£ (ä¾‹å¦‚ ILLMPermissionRequest)**ï¼Œè¿™æ · OCR/VLM/Embedding æ’ä»¶éƒ½å¯ä»¥ç»Ÿä¸€å£°æ˜ä¾èµ–ï¼Ÿ
